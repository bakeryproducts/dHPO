{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.append('/home/sokolov/work/cycler/dHPO/exp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "from functools import partial\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "from config import cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker_list(seq, size):\n",
    "    return (seq[i::size] for i in range(size))\n",
    "\n",
    "def parse_pool(pool_str):\n",
    "        pool_str = pool_str.split('_')[-1]\n",
    "        return ','.join([i for i in pool_str if i.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dag base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dagger:\n",
    "    def __init__(self, cfg, dag):\n",
    "        self.dag = dag\n",
    "        self.default_pool = 'CHANGE_ME'\n",
    "        \n",
    "    def create_task(self, name, func, pool=self.default_pool, op_kwargs=None):\n",
    "        task = PythonOperator(\n",
    "                        task_id=name,\n",
    "                        python_callable=func,\n",
    "                        op_kwargs=op_kwargs,\n",
    "                        provide_context=True,\n",
    "                        pool=pool,\n",
    "                        retries=self.cfg.DAG.RETRIES,\n",
    "                        retry_delay=timedelta(minutes=self.cfg.DAG.RETRY_DELAY),\n",
    "                        dag=self.dag,)\n",
    "        return task\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pull_results(self, **context):\n",
    "        results = None\n",
    "        if context is not None:\n",
    "            task_ids = context['dag'].task_ids\n",
    "            results = {task_id:context['ti'].xcom_pull(task_ids=task_id) for task_id in task_ids}\n",
    "        return results\n",
    "\n",
    "    def upstream_results(**context):\n",
    "        upstream_task_ids = context['task'].upstream_task_ids\n",
    "        rs = [pull_results(**context)[task_id] for task_id in upstream_task_ids]\n",
    "        if not rs: rs = [{'configs':None, 'docker_results':None}]\n",
    "        for result in rs:\n",
    "            yield result\n",
    "\n",
    "def dw_cycle_param(func, **context):\n",
    "    prev_results = next(upstream_results(**context))\n",
    "    aux_cfg_files = prev_results['configs']\n",
    "    gpus = parse_pool(context['ti'].pool)\n",
    "    return func(aux_cfg_files=aux_cfg_files, gpus=gpus)\n",
    "\n",
    "def dw_bo_param(func, **context):\n",
    "    results = pull_results(**context)\n",
    "    all_hp_points = []\n",
    "    for task_id, r in results.items():\n",
    "        if r:\n",
    "            all_hp_points.append({'points':r['state'], 'target':r['docker_results']['metric']})  \n",
    "    \n",
    "    prev_results = next(upstream_results(**context))\n",
    "    aux_cfg_files = prev_results['configs']\n",
    "    gpus = parse_pool(context['ti'].pool)\n",
    "    idx = cfg.GPUS.IDS.index(int(gpus))# wont work with single exp on multiple gpus!\n",
    "    return func(aux_cfg_files=aux_cfg_files, gpus=gpus, hp_points=all_hp_points, idx=idx)\n",
    "\n",
    "def dw_pooling(num=1, **context):\n",
    "    key = 'metric'\n",
    "    res = {}\n",
    "    for r in upstream_results(**context):\n",
    "        res[r['docker_results'][key]] = r['configs']\n",
    "    res = sorted(res.items(), key=lambda x: x[0], reverse=True)\n",
    "    res = res[:num]\n",
    "    best_docker_results = [r[0] for r in res]\n",
    "    best_configs = [r[1] for r in res]\n",
    "    print(f'\\n\\tPooling: best result : {best_docker_results}, {best_configs}\\n')\n",
    "    r = [{'configs':best_config} for best_config in best_configs]\n",
    "    return r if len(r)>1 else r[0]\n",
    "\n",
    "def dw_dist(idx=None, **context):\n",
    "    res = list(upstream_results(**context))[0][idx]\n",
    "    return res\n",
    "\n",
    "def distribute(name, num):\n",
    "    tasks = []\n",
    "    for i in range(num):\n",
    "        task_name = f'distribute_{name}_{i}'\n",
    "        tasks.append(create_task(task_name, dw_dist, op_kwargs={'idx':i}))\n",
    "    return tasks\n",
    "\n",
    "def block_optimize(n, name, func, dw_param):\n",
    "    tasks = []\n",
    "    gpus_avail = cycle(cfg.GPUS.IDS)\n",
    "\n",
    "    for i in range(n):\n",
    "        gpu = next(gpus_avail)\n",
    "        task_name = f'{name}_{i}'\n",
    "        pool = cfg.DAG.POOL_PREFIX + str(gpu)\n",
    "        \n",
    "        block_func = partial(func, seq_id=i, name=task_name)\n",
    "        task = create_task(task_name, dw_param, pool=pool, op_kwargs={'func':block_func}) \n",
    "        tasks.append(task)\n",
    "    return tasks\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = days_ago(1)# + timedelta(hours=10, minutes=31)\n",
    "\n",
    "default_args = {\n",
    "    'owner': cfg.OWNER,\n",
    "    'depends_on_past': False,\n",
    "    #'start_date':d,\n",
    "    'email': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    #'retries': 0,# overrided in pythonOperator down below\n",
    "    #'retry_delay': timedelta(minutes=5),# overrided in pythonOperator down below\n",
    "}\n",
    "\n",
    "default_pool = cfg.DAG.DEF_POOL\n",
    "#schedule_interval = cfg.DAG.SCHED_INTERVAL if cfg.DAG.SCHED_INTERVAL else None #@daily\n",
    "description = cfg.DAG.DESC + '\\n' + json.dumps(cfg, indent=4)\n",
    "\n",
    "schedule_interval = '00 22 * * *'\n",
    "\n",
    "dag = DAG(  cfg.DAG.NAME,\n",
    "                max_active_runs=1,\n",
    "                start_date=d,\n",
    "                catchup=False,# for disabling backfill!\n",
    "                description=description,\n",
    "                schedule_interval=schedule_interval,\n",
    "                default_args=default_args,\n",
    "                )\n",
    "\n",
    "def base_task_generator(name, func, dag, pool=default_pool, op_kwargs=None):\n",
    "    task = PythonOperator(\n",
    "                    task_id=name,\n",
    "                    python_callable=func,\n",
    "                    op_kwargs=op_kwargs,\n",
    "                    provide_context=True,\n",
    "                    pool=pool,\n",
    "                    retries=cfg.DAG.RETRIES,\n",
    "                    retry_delay=timedelta(minutes=cfg.DAG.RETRY_DELAY),\n",
    "                    dag=dag,)\n",
    "    return task\n",
    "        \n",
    "create_task = partial(base_task_generator, dag=dag)\n",
    "\n",
    "def pull_results(**context):\n",
    "    results = None\n",
    "    if context is not None:\n",
    "        task_ids = context['dag'].task_ids\n",
    "        results = {task_id:context['ti'].xcom_pull(task_ids=task_id) for task_id in task_ids}\n",
    "    return results\n",
    "\n",
    "\n",
    "def upstream_results(**context):\n",
    "    task = context['task']\n",
    "    upstream_task_ids = task.upstream_task_ids\n",
    "    rs = [pull_results(**context)[task_id] for task_id in upstream_task_ids]\n",
    "    if not rs:\n",
    "        rs = [{'configs':None, 'docker_results':None}]\n",
    "    for result in rs:\n",
    "        yield result        \n",
    "\n",
    "def parse_pool(pool_str):\n",
    "    pool_str = pool_str.split('_')[-1]\n",
    "    return ','.join([i for i in pool_str if i.isdigit()])\n",
    "\n",
    "def chunker_list(seq, size):\n",
    "    return (seq[i::size] for i in range(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dag extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dw_cycle_param(func, **context):\n",
    "    prev_results = next(upstream_results(**context))\n",
    "    aux_cfg_files = prev_results['configs']\n",
    "    gpus = parse_pool(context['ti'].pool)\n",
    "    return func(aux_cfg_files=aux_cfg_files, gpus=gpus)\n",
    "\n",
    "def dw_bo_param(func, **context):\n",
    "    results = pull_results(**context)\n",
    "    all_hp_points = []\n",
    "    for task_id, r in results.items():\n",
    "        if r:\n",
    "            all_hp_points.append({'points':r['state'], 'target':r['docker_results']['metric']})  \n",
    "    \n",
    "    prev_results = next(upstream_results(**context))\n",
    "    aux_cfg_files = prev_results['configs']\n",
    "    gpus = parse_pool(context['ti'].pool)\n",
    "    idx = cfg.GPUS.IDS.index(int(gpus))# wont work with single exp on multiple gpus!\n",
    "    return func(aux_cfg_files=aux_cfg_files, gpus=gpus, hp_points=all_hp_points, idx=idx)\n",
    "\n",
    "def dw_pooling(num=1, **context):\n",
    "    key = 'metric'\n",
    "    res = {}\n",
    "    for r in upstream_results(**context):\n",
    "        res[r['docker_results'][key]] = r['configs']\n",
    "    res = sorted(res.items(), key=lambda x: x[0], reverse=True)\n",
    "    res = res[:num]\n",
    "    best_docker_results = [r[0] for r in res]\n",
    "    best_configs = [r[1] for r in res]\n",
    "    print(f'\\n\\tPooling: best result : {best_docker_results}, {best_configs}\\n')\n",
    "    r = [{'configs':best_config} for best_config in best_configs]\n",
    "    return r if len(r)>1 else r[0]\n",
    "\n",
    "def dw_dist(idx=None, **context):\n",
    "    res = list(upstream_results(**context))[0][idx]\n",
    "    return res\n",
    "\n",
    "def distribute(name, num):\n",
    "    tasks = []\n",
    "    for i in range(num):\n",
    "        task_name = f'distribute_{name}_{i}'\n",
    "        tasks.append(create_task(task_name, dw_dist, op_kwargs={'idx':i}))\n",
    "    return tasks\n",
    "        \n",
    "def block_optimize(n, name, func, dw_param):\n",
    "    tasks = []\n",
    "    gpus_avail = cycle(cfg.GPUS.IDS)\n",
    "        \n",
    "    for i in range(n):\n",
    "        gpu = next(gpus_avail)\n",
    "        task_name = f'{name}_{i}'\n",
    "        pool = cfg.DAG.POOL_PREFIX + str(gpu)\n",
    "        \n",
    "        block_func = partial(func, seq_id=i, name=task_name)\n",
    "        task = create_task(task_name, dw_param, pool=pool, op_kwargs={'func':block_func}) \n",
    "        tasks.append(task) \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "t1 = block_optimize(1, 'rs_exp', cycle_exp, dw_cycle_param)\n",
    "t2 = block_optimize(1, 'rs_mut', cycle_mutate, dw_cycle_param)\n",
    "t1[0] >> t2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "block_optimize(2, 'rs_all', cycle_all, dw_cycle_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallel chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "tasks = {}\n",
    "tasks['exp'] = block_optimize(3, 'cycle_e', cycle_exp, dw_cycle_param)\n",
    "tasks['mut'] = block_optimize(3, 'cycle_m', cycle_mutate, dw_cycle_param)\n",
    "for task_e, task_m in zip(tasks['exp'], tasks['mut']):\n",
    "    task_e >> task_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pooling one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "tasks = {}\n",
    "tasks['exp'] = block_optimize(2, 'cycle_e', cycle_exp, dw_cycle_param)\n",
    "pooling_task1 = create_task(f'pooling1', dw_pooling) \n",
    "tasks['exp'] >> pooling_task1\n",
    "\n",
    "tasks['mut'] = block_optimize(2, 'cycle_m', cycle_mutate, dw_cycle_param)\n",
    "pooling_task1 >> tasks['mut']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pooling many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "tasks = {}\n",
    "#block_optimize(50, 'cycle_all', cycle_all, dw_cycle_param)\n",
    "tasks['exp'] = block_optimize(3, 'cycle_e', cycle_exp, dw_cycle_param)\n",
    "n_pooling = 2\n",
    "pooling_task1 = create_task(f'pooling_{n_pooling}_best', partial(dw_pooling, num=n_pooling))\n",
    "tasks['exp'] >> pooling_task1\n",
    "\n",
    "dist_tasks = distribute('best_exp', n_pooling)\n",
    "pooling_task1 >> dist_tasks\n",
    "\n",
    "mut_tasks = block_optimize(5, 'cycle_m', cycle_mutate, dw_cycle_param)\n",
    "for task_d, task_mut in zip(dist_tasks, chunker_list(mut_tasks, n_pooling)):\n",
    "    task_d >> task_mut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "tasks = {}\n",
    "tasks['all'] = block_optimize(10, 'cycle_all', cycle_all, dw_cycle_param)\n",
    "\n",
    "dist_num = 5\n",
    "pooling_task1 = create_task(f'pooling_{dist_num}_best1', partial(dw_pooling, num=dist_num))\n",
    "tasks['all'] >> pooling_task1\n",
    "\n",
    "dist_tasks = distribute('best_all', dist_num)\n",
    "pooling_task1 >> dist_tasks\n",
    "\n",
    "tasks['mut'] = block_optimize(25, 'cycle_m', cycle_mutate, dw_cycle_param)\n",
    "for dt, mt in zip(dist_tasks, chunker_list(tasks['mut'], dist_num)):\n",
    "    dt >> mt\n",
    "\n",
    "pooling_task2 = create_task(f'pooling_{dist_num}_best2', partial(dw_pooling, num=dist_num))\n",
    "tasks['mut'] >> pooling_task2\n",
    "\n",
    "dist_tasks2 = distribute('best_mut', dist_num)\n",
    "pooling_task2 >> dist_tasks2\n",
    "\n",
    "\n",
    "tasks['cr'] = block_optimize(10, 'cycle_cr', cycle_crossover, dw_cycle_param)\n",
    "for dt, mt in zip(dist_tasks2, chunker_list(tasks['cr'], dist_num)):\n",
    "    dt >> mt\n",
    "\n",
    "pooling_task3 = create_task(f'pooling3', dw_pooling) \n",
    "tasks['cr'] >> pooling_task3\n",
    "\n",
    "tasks['co'] = block_optimize(5, 'cycle_co', cycle_combine, dw_cycle_param)\n",
    "pooling_task3 >> tasks['co']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "tasks_bo_all = block_optimize(3, 'bo_all', bo_all, dw_bo_param)\n",
    "\n",
    "pooling_task1 = create_task(f'pooling1', dw_pooling) \n",
    "tasks_bo_all >> pooling_task1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import TaskInstance\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test1(**context):\n",
    "    pprint(context)\n",
    "    return 55\n",
    "\n",
    "def test2(**context):\n",
    "    pprint(context)\n",
    "    pprint(pull_results2(**context))\n",
    "    return 999\n",
    "\n",
    "\n",
    "def pull_results2(**context):\n",
    "    results = None\n",
    "    if context is not None:\n",
    "        task = context['task']\n",
    "        dag = context['dag']\n",
    "        task_ids = dag.task_ids\n",
    "        #task_ids = task.upstream_task_ids\n",
    "        for task_id in task_ids:\n",
    "            print(task_id)\n",
    "            xc = context['ti'].xcom_pull(task_ids=task_id)\n",
    "            print(xc)\n",
    "            \n",
    "        results = {task_id:context['ti'].xcom_pull(task_ids=task_id) for task_id in task_ids}\n",
    "    return results\n",
    "\n",
    "task1 = base_task_generator('test1', test1, dag)\n",
    "task2 = base_task_generator('test2', test2, dag)\n",
    "#task1 >> task2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti = TaskInstance(task=task2, execution_date=datetime.now())\n",
    "# ctxt = ti.get_template_context()\n",
    "# task2.execute(context=ctxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ctxt['dag']\n",
    "t = d.task_dict['test2']\n",
    "t.xcom_pull(ctxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2.xcom_push(ctxt, 'test',13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1.xcom_pull(ctxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.pool, ti.current_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = TaskInstance(task=task1, execution_date=datetime.now())\n",
    "task1.execute(context=ti.get_template_context())\n",
    "ti.pool, ti.current_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.pool, ti.current_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_state(args):\n",
    "    dag = get_dag(args)\n",
    "    task = dag.get_task(task_id=args.task_id)\n",
    "    ti = TaskInstance(task, args.execution_date)\n",
    "    print(ti.current_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow list_dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
